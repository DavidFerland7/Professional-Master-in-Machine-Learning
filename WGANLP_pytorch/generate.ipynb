{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set seed\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "#import\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch import autograd\n",
    "import os\n",
    "import scipy.misc\n",
    "from scipy.misc import imsave\n",
    "from datetime import datetime\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "#import modules\n",
    "from sampler import svhn_sampler\n",
    "from model import Critic, Generator\n",
    "from train import vf_wasserstein_distance, save_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizeInverse(torchvision.transforms.Normalize):\n",
    "    \"\"\"\n",
    "    Undoes the normalization and returns the reconstructed images in the input domain.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mean, std):\n",
    "        mean = torch.as_tensor(mean)\n",
    "        std = torch.as_tensor(std)\n",
    "        std_inv = 1 / (std + 1e-7)\n",
    "        mean_inv = -mean * std_inv\n",
    "        super().__init__(mean=mean_inv, std=std_inv)\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        return super().__call__(tensor.clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 64\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "z_dim = 100\n",
    "\n",
    "root = './'\n",
    "model_dir_relpath = 'models'\n",
    "model_run_relpath = 'output_2020_04_27__01_51_05'\n",
    "\n",
    "model_path = os.path.join(root, model_dir_relpath, model_run_relpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic = torch.load(model_path + 'critic.pt', map_location=torch.device('cpu'))\n",
    "generator = torch.load(model_path + 'generator.pt', map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output_relpath = os.path.join('output', 'samples')\n",
    "\n",
    "model_output_path = os.path.join(model_output_path, model_output_relpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.randn(train_batch_size, z_dim, device = device)\n",
    "z = autograd.Variable(z, requires_grad=False)\n",
    "samples = generator(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unorm = NormalizeInverse(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "samples_norm = torch.stack([unorm(x) for x in samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create directory if not exist\n",
    "samples_norm = samples_norm.data.numpy()\n",
    "os.makedirs(model_output_path, exist_ok=True)\n",
    "\n",
    "save_images(\n",
    "    samples_norm,\n",
    "    os.path.join(model_output_path, 'samples.png')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze images from perturbations in z space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output_relpath = os.path.join('output', 'latent_space_variations')\n",
    "\n",
    "model_output_path = os.path.join(model_output_path, model_output_relpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "image_num = 0\n",
    "eps_list = [-7.5, -5, -2.5, 0, 2.5, 5, 7.5]\n",
    "for dim in range(z_dim):\n",
    "    \n",
    "    z_img_alleps = []\n",
    "    for eps in eps_list:\n",
    "        torch.manual_seed(0)\n",
    "        z_all = torch.randn(train_batch_size, z_dim, device = device)\n",
    "        z_img = z_all[image_num, :]\n",
    "        z_img[dim] += eps\n",
    "        z_img_alleps.append(z_img)\n",
    "    if len(z_img_alleps) == 0:\n",
    "        z_img_alleps = z_img_alleps.unsqueeze(0)\n",
    "    else:\n",
    "        z_img_alleps = torch.stack(z_img_alleps)\n",
    "    z_img_alleps = autograd.Variable(z_img_alleps, requires_grad=False)\n",
    "    \n",
    "    samples = []\n",
    "    for z in z_img_alleps:\n",
    "        test = generator(z.unsqueeze(0))\n",
    "        torch.manual_seed(0)\n",
    "        samples.append(generator(z.unsqueeze(0)).squeeze())\n",
    "    if len(samples) == 0:\n",
    "        samples = samples.unsqueeze(0)\n",
    "    else:\n",
    "        samples = torch.stack(samples)\n",
    "    \n",
    "    samples_norm = []\n",
    "    for s in samples:\n",
    "        samples_norm.append(unorm(s))\n",
    "    if len(samples_norm) == 0:\n",
    "        samples_norm = samples_norm.unsqueeze(0)\n",
    "    else:\n",
    "        samples_norm = torch.stack(samples_norm)\n",
    "        \n",
    "    samples_norm = samples_norm.data.numpy()\n",
    "    \n",
    "    ### Output ###\n",
    "    os.makedirs(model_output_path, exist_ok=True)\n",
    "    save_images(\n",
    "        samples_norm,\n",
    "        os.path.join(model_output_path, 'samples_dim_{}.png'.format(dim))\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
