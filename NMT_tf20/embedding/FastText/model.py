import os
import sys

sys.path.insert(0, os.path.join(os.path.dirname(os.path.abspath(__file__)), "..", ".."))

from embedding.BaseEmbedding import BaseEmbedding
import fasttext
from fasttext.util import find_nearest_neighbor
import json
import os
import numpy as np
import globals as globals_vars
from gensim.models.fasttext import FastText as FT_gensim
from gensim.utils import RULE_KEEP


class FastText(BaseEmbedding):
    def __init__(self, lang, config_path=None):
        if config_path is None:
            config_path = os.path.dirname(os.path.abspath(__file__)) + "/config.json"

        super().__init__(config_path)  # self.config created in super()
        self.lang = lang
        self.emb_size = self.config["embedding_model"][self.lang]["emb_size"]
        self.model = FT_gensim(
            size=self.emb_size,
            sg=0,
            bucket=self.config["embedding_model"][self.lang]["bucket"],
            negative=self.config["embedding_model"][self.lang]["neg_sample"],
        )

        dict_to_hash = {
            "embedding": self.config["embedding_model"][self.lang],
            "preprocessing_unaligned": self.config["preprocessing"]["raw_unaligned"][
                self.lang
            ],
            "preprocessing_aligned": self.config["preprocessing"]["raw_aligned"][
                self.lang
            ],
            "max_obs": self.config["preprocessing"].get("max_obs", None),
        }
        self.config_hash = super().hash_config(dict_to_hash)

        if os.path.isdir(globals_vars.PATH_SERVER):

            parent_dir_name = os.path.join(
                os.path.split(
                    os.path.split(os.path.dirname(os.path.abspath(__file__)))[0]
                )[-1],
                os.path.split(os.path.dirname(os.path.abspath(__file__)))[-1],
            )
            self.path_cache = os.path.join(
                globals_vars.PATH_SERVER, parent_dir_name, self.lang
            )
        else:
            self.path_cache = os.path.join(
                os.path.dirname(os.path.abspath(__file__)), self.lang
            )

    def _rule(self, word, count, min_count):  # params are needed
        return RULE_KEEP

    def fit(self, df):

        super().fit(df, self.lang)  ## get vocab from base_embedding

        df.loc[:, "text"] = df["text"].apply(lambda x: x.split())

        # build the vocabulary from word_to_idx generated by base_embedding.
        self.model.build_vocab(
            sentences=[self.word_to_idx.keys()], trim_rule=self._rule
        )

        # train the model
        self.model.train(
            sentences=df.text.values.tolist(),
            epochs=self.model.epochs,
            total_examples=self.model.corpus_count,
            total_words=self.model.corpus_total_words,
        )

    def generate(self, df):
        # This function returns word ID as per training mapping ( this is useful when we need to fine tune model)
        # When a word is not in the vocabulary we return UNK

        if not len(self.model.wv.vocab):
            self._load_model()

        sentences = []
        for sent in df.text.values.tolist():
            if type(sent) == float:
                sent = ""
            sent_tok = [
                self.word_to_idx.get(tok, self.word_to_idx["UNK"])
                for tok in sent.split()
            ]
            sentences.append(sent_tok)
        return sentences

    def generate_emb(self, df):
        # This function returns embedding vectors directly if needed
        if not len(self.model.wv.vocab):
            self._load_model()

        sentences = []
        for sent in df.text.values.tolist():
            if type(sent) == float:
                sent = ""
            sent_tok = sent.split()
            sentences.append(self.model.wv[sent_tok] if sent_tok else [])
        return sentences

    def save_model(self):

        super().save_model(os.path.join(self.path_cache, self.config_hash))

        self.model.save(os.path.join(self.path_cache, self.config_hash, "model.bin"))

    def _load_model(self):
        super()._load_model(os.path.join(self.path_cache, self.config_hash))
        self.model = FT_gensim.load(
            os.path.join(self.path_cache, self.config_hash, "model.bin")
        )

    def find_nearest_neigborhs(self, word):
        id = find_nearest_neighbor(
            self.model.get_word_vector(word), self.model.get_output_matrix(), ban_set=[]
        )

        dict_word = self.model.get_words()

        return dict_word[id]


if __name__ == "__main__":
    #
    # from preprocessing.TxtPreprocessor import run_txt_preprocessor
    #
    # config_path = os.path.dirname(os.path.abspath(__file__)) + "/config.json"
    # config = {}
    # if config_path:
    #     assert os.path.isfile(config_path), f"invalid user config file: {config_path}"
    #     with open(config_path) as fd:
    #         config = json.load(fd)
    # lang = "en"
    #
    # df_en = run_txt_preprocessor(
    #     {"preprocessing": config["preprocessing"]}, "raw_aligned", "en"
    # )
    # df_en = df_en.append(
    #     run_txt_preprocessor(
    #         {"preprocessing": config["preprocessing"]},
    #         "raw_unaligned",
    #         "en",
    #     )
    # )
    # print("head df_en AFTER:\n ", df_en.head(100))

    emb = FastText("fr")
    emb._load_model()
    emb.model.wv.most_similar(positive=["BOS"], topn=5)
    # test = emb.generate(df_en)
    print("test")
