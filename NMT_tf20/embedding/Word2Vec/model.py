import os
import sys

sys.path.insert(0, os.path.join(os.path.dirname(os.path.abspath(__file__)), "..", ".."))

from embedding.BaseEmbedding import BaseEmbedding
import json
import os
import numpy as np
import globals as globals_vars
from gensim import models
from gensim.utils import RULE_KEEP
from gensim.models.callbacks import CallbackAny2Vec


class callback(CallbackAny2Vec):
    """Callback to print loss after each epoch."""

    def __init__(self):
        self.epoch = 0
        self.loss_to_be_subed = 0

    def on_epoch_end(self, model):
        loss = model.get_latest_training_loss()
        loss_now = loss - self.loss_to_be_subed
        self.loss_to_be_subed = loss
        print("W2V training: Loss after epoch {}: {}".format(self.epoch, loss_now))
        self.epoch += 1


class Word2Vec(BaseEmbedding):
    def __init__(self, lang, config_path=None):
        if config_path is None:
            config_path = os.path.dirname(os.path.abspath(__file__)) + "/config.json"

        super().__init__(config_path)  # self.config created in super()
        self.lang = lang
        self.emb_size = self.config["embedding_model"][self.lang]["model_kwargs"][
            "size"
        ]
        self.model = models.Word2Vec(
            compute_loss=True,
            callbacks=[callback()],
            **self.config["embedding_model"][self.lang]["model_kwargs"],
        )

        print("\nmbeding config for lang ({})\n{}".format(self.lang, self.config))

        dict_to_hash = {
            "embedding": self.config["embedding_model"][self.lang],
            "preprocessing_unaligned": self.config["preprocessing"]["raw_unaligned"][
                self.lang
            ],
            "preprocessing_aligned": self.config["preprocessing"]["raw_aligned"][
                self.lang
            ],
            "max_obs": self.config["preprocessing"].get("max_obs", None),
        }
        self.config_hash = super().hash_config(dict_to_hash)

        if os.path.isdir(globals_vars.PATH_SERVER):

            parent_dir_name = os.path.join(
                os.path.split(
                    os.path.split(os.path.dirname(os.path.abspath(__file__)))[0]
                )[-1],
                os.path.split(os.path.dirname(os.path.abspath(__file__)))[-1],
            )
            self.path_cache = os.path.join(
                globals_vars.PATH_SERVER, parent_dir_name, self.lang
            )
        else:
            self.path_cache = os.path.join(
                os.path.dirname(os.path.abspath(__file__)), self.lang
            )

    def _rule(self, word, count, min_count):  # params are needed
        return RULE_KEEP

    def fit(self, df):

        super().fit(df, self.lang)  ## get vocab from base_embedding

        df.loc[:, "text"] = df["text"].apply(lambda x: x.split())

        # build the vocabulary from word_to_idx generated by base_embedding.
        self.model.build_vocab(
            sentences=[self.word_to_idx.keys()], trim_rule=self._rule
        )

        # train the model
        self.model.train(
            sentences=df.text.values.tolist(),
            epochs=self.model.epochs,
            total_examples=self.model.corpus_count,
            total_words=self.model.corpus_total_words,
            compute_loss=True,
            callbacks=[callback()],
        )

    def generate(self, df):
        # This function returns word ID as per training mapping ( this is useful when we need to fine tune model)
        # When a word is not in the vocabulary we return <UNK>

        if not len(self.model.wv.vocab):
            self._load_model()

        sentences = []
        for sent in df.text.values.tolist():
            if type(sent) == float:
                sent = ""
            sent_tok = [
                self.word_to_idx.get(tok, self.word_to_idx["<UNK>"])
                for tok in sent.split()
            ]
            sentences.append(sent_tok)
        return sentences

    def generate_emb(self, df):
        # This function returns embedding vectors directly if needed
        if not len(self.model.wv.vocab):
            self._load_model()

        sentences = []
        for sent in df.text.values.tolist():
            if type(sent) == float:
                sent = ""
            sent_tok = sent.split()
            emb_tok = []
            for tok in sent_tok:
                if tok in self.model.wv:
                    emb_tok.append(self.model.wv[tok])
                else:
                    emb_tok.append(self.model.wv["<UNK>"])
            sentences.append(emb_tok)
        return sentences

    def generate_emb_from_list(self, list):
        # This function returns embedding vectors directly if needed
        if not len(self.model.wv.vocab):
            self._load_model()

        sentences = []
        for sent in list:
            if type(sent) == float:
                sent = ""
            sent_tok = sent.split()
            emb_tok = []
            for tok in sent_tok:
                if tok in self.model.wv:
                    emb_tok.append(self.model.wv[tok])
                else:
                    emb_tok.append(self.model.wv["<UNK>"])
            sentences.append(emb_tok)
        return sentences

    def save_model(self):

        super().save_model(os.path.join(self.path_cache, self.config_hash))

        self.model.save(os.path.join(self.path_cache, self.config_hash, "model.bin"))

    def _load_model(self):
        super()._load_model(os.path.join(self.path_cache, self.config_hash))
        self.model = models.Word2Vec.load(
            os.path.join(self.path_cache, self.config_hash, "model.bin")
        )


if __name__ == "__main__":

    from preprocessing.TxtPreprocessor import run_txt_preprocessor

    config_path = os.path.dirname(os.path.abspath(__file__)) + "/config.json"
    config = {}
    if config_path:
        assert os.path.isfile(config_path), f"invalid user config file: {config_path}"
        with open(config_path) as fd:
            config = json.load(fd)
    lang = "en"

    df_en = run_txt_preprocessor(
        {"preprocessing": config["preprocessing"]}, "raw_aligned", "en"
    )
    df_en = df_en.append(
        run_txt_preprocessor(
            {"preprocessing": config["preprocessing"]}, "raw_unaligned", "en",
        )
    )
    print("head df_en AFTER:\n ", df_en.head(100))

    emb = Word2Vec("en")
    test = emb.generate(df_en)
    print(test)
